cfg: config/Octavius/octavius_2d_e4_bs64.yaml
conv_template: default
data_path: null
data_path_2d: /data/xiaochen/FedMFM/preprocessed_jsons/medqa_server.json
data_path_3d: ../data/Octavius/3D_Instruct/meta_file/scan2inst_train.json
decoding_method: sampling
deepspeed:
  activation_checkpointing:
    contiguous_memory_optimization: false
    cpu_checkpointing: true
    number_checkpoints: null
    partition_activations: true
    profile: false
    synchronize_checkpoint_boundary: false
  bf16:
    enable: false
  fp16:
    enabled: true
    hysteresis: 2
    initial_scale_power: 12
    loss_scale_window: 1000
    min_loss_scale: 1
    opt_level: O2
  gradient_accumulation_steps: 32
  gradient_clipping: 1
  optimizer:
    params:
      betas:
      - 0.9
      - 0.95
      eps: 1.0e-06
      lr: 0.0005
      weight_decay: 0.0001
    type: Adam
  scheduler:
    params:
      total_num_steps: 720
      warmup_max_lr: 0.0005
      warmup_min_lr: 0
      warmup_num_steps: 72
    type: WarmupDecayLR
  steps_per_print: 1
  train_batch_size: 128
  train_micro_batch_size_per_gpu: 4
  zero_optimization:
    allgather_bucket_size: 500000000
    allgather_partitions: true
    contiguous_gradients: true
    offload_optimizer:
      device: cpu
    stage: 1
delta_ckpt_path: null
dschf: !!python/object:transformers.deepspeed.HfDeepSpeedConfig
  _offload: false
  _stage: 1
  config:
    activation_checkpointing:
      contiguous_memory_optimization: false
      cpu_checkpointing: true
      number_checkpoints: null
      partition_activations: true
      profile: false
      synchronize_checkpoint_boundary: false
    bf16:
      enable: false
    fp16:
      enabled: true
      hysteresis: 2
      initial_scale_power: 12
      loss_scale_window: 1000
      min_loss_scale: 1
      opt_level: O2
    gradient_accumulation_steps: 32
    gradient_clipping: 1
    optimizer:
      params:
        betas:
        - 0.9
        - 0.95
        eps: 1.0e-06
        lr: 0.0005
        weight_decay: 0.0001
      type: Adam
    scheduler:
      params:
        total_num_steps: 20000
        warmup_max_lr: 0.0005
        warmup_min_lr: 0
        warmup_num_steps: 10
      type: WarmupDecayLR
    steps_per_print: 1
    train_batch_size: 128
    train_micro_batch_size_per_gpu: 4
    zero_optimization:
      allgather_bucket_size: 500000000
      allgather_partitions: true
      contiguous_gradients: true
      offload_optimizer:
        device: cpu
      stage: 1
encoder_ckpt_path: null
encoder_pretrain: clip
epochs: 10
find_unused_parameters: true
generate_len: 128
gradient_checkpointing: false
llm_ckpt_path: /data/xiaochen/FedMFM/MMedLM2/
llm_proj_path: null
local_rank: 1
log_path: ../ckpt/octavius_2d_e4_bs64/log_rest/
logging_step: 5
loop_2d: 1
loop_3d: 0
lora_alpha: 32
lora_dropout: 0.1
lora_r: 8
lora_target_modules:
- wqkv
- wo
- w1
- w2
- w3
- output
master_ip: 127.0.0.1
master_port: '2337'
max_len: 256
max_length: 256
max_shard_size: 10GB
max_tgt_len: 128
mode: train
model: octavius
models:
  octavius:
    agent_name: DeepSpeedAgent
    model_name: Octavius
    stage1_train_dataset: OctaviusDataset
    test_dataset: SelfInstructTestDataset
moe_gate_mode: top2_gate
moe_lora_num_experts: 8
num_clients: '5'
num_points: 40000
num_vision_token: 198
octavius_modality:
- image
peft_type: moe_lora
penalty_alpha: 0.6
random_prefix_len: 5
root_dir: ../
sample_num: 2
save_path: /data/xiaochen/FedMFM/ckpt/ours_no_task
seed: 42
stage: 1
top_k: 1
top_p: 0.1
total_steps: 720
use_color: false
use_flash_attn: false
use_height: false
use_system: true
use_xformers: false
vision_feature_type: local
vision_output_layer: -1
vision_root_path: null
vision_root_path_2d: ../data/Octavius/2D_Instruct
vision_root_path_3d: ../data/Octavius/3D_Instruct
vision_type: image
warmup_rate: 0.1
world_size: 1
