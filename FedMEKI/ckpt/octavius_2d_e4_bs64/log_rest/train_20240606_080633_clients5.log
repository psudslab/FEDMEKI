/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Setting ds_accelerator to cuda (auto detect)
/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:25: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
WARNING:root:Could not import _ext module.
Please see the setup instructions in the README: https://github.com/erikwijmans/Pointnet2_PyTorch/blob/master/README.rst. Please refer to README.md to install optional extension for 3D environment if required.
WARNING:root:No module named 'lightllm'. Please refer to README.md to install optional LightLLM extension if required.
Arguments: 
{
    "cfg": "config/Octavius/octavius_2d_e4_bs64.yaml",
    "conv_template": "default",
    "data_path": null,
    "delta_ckpt_path": null,
    "encoder_ckpt_path": null,
    "encoder_pretrain": "clip",
    "gradient_checkpointing": false,
    "llm_ckpt_path": "/data/xiaochen/FedMFM/MMedLM2/",
    "llm_proj_path": null,
    "local_rank": 0,
    "log_path": "../ckpt/octavius_2d_e4_bs64/log_rest/",
    "max_tgt_len": 128,
    "model": "octavius",
    "num_clients": "5",
    "num_points": 40000,
    "num_vision_token": 198,
    "save_path": "/data/xiaochen/FedMFM/ckpt/ours_fedavg_image",
    "stage": 1,
    "use_color": false,
    "use_flash_attn": false,
    "use_height": false,
    "use_system": true,
    "use_xformers": false,
    "vision_feature_type": "local",
    "vision_output_layer": -1,
    "vision_root_path": null,
    "vision_type": "image"
}
[2024-06-06 08:06:39,142] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-06-06 08:06:39,142] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-06-06 08:06:39,142] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Initializing [clip] visual encoder from ~/.cache/clip/ViT-L-14.pt [cuda]...
/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/transformers/models/deit/feature_extraction_deit.py:28: FutureWarning: The class DeiTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DeiTImageProcessor instead.
  warnings.warn(
Visual encoder initialized.
Initializing language decoder from /data/xiaochen/FedMFM/MMedLM2/ ...
Build PEFT model with LoRA-MoE.
loading medical ckpt
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:07<00:22,  7.46s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:14<00:14,  7.10s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:22<00:07,  7.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:23<00:00,  5.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:23<00:00,  5.90s/it]
InternLM2ForCausalLM(
  (model): InternLM2Model(
    (tok_embeddings): Embedding(92544, 4096, padding_idx=2)
    (layers): ModuleList(
      (0): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (1): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (2): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (3): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (4): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (5): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (6): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (7): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (8): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (9): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (10): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (11): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (12): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (13): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (14): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (15): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (16): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (17): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (18): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (19): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (20): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (21): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (22): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (23): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (24): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (25): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (26): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (27): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (28): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (29): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (30): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (31): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
    )
    (norm): InternLM2RMSNorm()
  )
  (output): Linear(in_features=4096, out_features=92544, bias=False)
)
trainable params: 78589952 || all params: 7816298496 || trainable%: 1.0054625221928064
Language decoder initialized.
Add VISION TAG ("<Img>" and "</Img>") for modality image.
Octavius Modalities: ['image']
Octavius 2D projection layer initialized.
Signal processing module initialized.
Clinical readings processing module initialized.
Demographics MLP encoder initialized.
[2024-06-06 08:10:21,564] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.3, git-hash=4e80e29, git-branch=HEAD
[2024-06-06 08:10:21,564] [INFO] [comm.py:619:init_distributed] Distributed backend already initialized
[2024-06-06 08:10:40,553] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/xmw5190/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Emitting ninja build file /home/xmw5190/.cache/torch_extensions/py310_cu113/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.5181074142456055 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000500, betas=(0.900000, 0.950000), weight_decay=0.000010, adam_w=1
[2024-06-06 08:10:44,755] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-06-06 08:10:44,908] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-06-06 08:10:44,908] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-06-06 08:10:44,909] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
[2024-06-06 08:10:44,909] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2024-06-06 08:10:44,909] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2024-06-06 08:10:44,909] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2024-06-06 08:10:44,909] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/xmw5190/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Emitting ninja build file /home/xmw5190/.cache/torch_extensions/py310_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.1438131332397461 seconds
Rank: 0 partition count [1] and sizes[(892644780, False)] 
[2024-06-06 08:10:49,768] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2024-06-06 08:10:49,769] [INFO] [utils.py:786:see_memory_usage] MA 15.88 GB         Max_MA 15.88 GB         CA 16.02 GB         Max_CA 16 GB 
[2024-06-06 08:10:49,769] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 77.69 GB, percent = 20.6%
[2024-06-06 08:10:53,294] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2024-06-06 08:10:53,295] [INFO] [utils.py:786:see_memory_usage] MA 15.88 GB         Max_MA 15.88 GB         CA 16.02 GB         Max_CA 16 GB 
[2024-06-06 08:10:53,295] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 88.6 GB, percent = 23.5%
[2024-06-06 08:10:53,295] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2024-06-06 08:10:53,411] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2024-06-06 08:10:53,413] [INFO] [utils.py:786:see_memory_usage] MA 15.88 GB         Max_MA 15.88 GB         CA 16.02 GB         Max_CA 16 GB 
[2024-06-06 08:10:53,413] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 88.62 GB, percent = 23.5%
[2024-06-06 08:10:53,430] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2024-06-06 08:10:53,430] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-06-06 08:10:53,430] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fcf1dc063e0>
[2024-06-06 08:10:53,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0005], mom=[[0.9, 0.95]]
[2024-06-06 08:10:53,435] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2024-06-06 08:10:53,435] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": true, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": true, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-06 08:10:53,435] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-06 08:10:53,435] [INFO] [config.py:964:print]   amp_enabled .................. False
[2024-06-06 08:10:53,435] [INFO] [config.py:964:print]   amp_params ................... False
[2024-06-06 08:10:53,435] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fcf1db323e0>
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   communication_data_type ...... None
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   disable_allgather ............ False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   dump_state ................... False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 128}
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   global_rank .................. 0
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 8
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   gradient_clipping ............ 1
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 65536
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   loss_scale ................... 0
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2024-06-06 08:10:53,436] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   optimizer_params ............. {'betas': [0.9, 0.95], 'eps': 1e-08, 'lr': 0.0005, 'weight_decay': 1e-05}
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   pld_enabled .................. False
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   pld_params ................... False
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 2881, 'warmup_max_lr': 0.0005, 'warmup_min_lr': 0, 'warmup_num_steps': 288}
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   sparse_attention ............. None
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   steps_per_print .............. 1
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   train_batch_size ............. 32
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  4
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   world_size ................... 1
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   zero_enabled ................. True
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-06 08:10:53,437] [INFO] [config.py:964:print]   zero_optimization_stage ...... 1
[2024-06-06 08:10:53,437] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 4, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1, 
    "steps_per_print": 1, 
    "zero_optimization": {
        "allgather_bucket_size": 5.000000e+08, 
        "allgather_partitions": true, 
        "contiguous_gradients": true, 
        "offload_optimizer": {
            "device": "cpu"
        }, 
        "stage": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "betas": [0.9, 0.95], 
            "eps": 1e-08, 
            "lr": 0.0005, 
            "weight_decay": 1e-05
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "total_num_steps": 2.881000e+03, 
            "warmup_max_lr": 0.0005, 
            "warmup_min_lr": 0, 
            "warmup_num_steps": 288
        }
    }, 
    "fp16": {
        "enabled": true, 
        "min_loss_scale": 128, 
        "opt_level": "O2"
    }, 
    "bf16": {
        "enable": false
    }, 
    "activation_checkpointing": {
        "partition_activations": true, 
        "cpu_checkpointing": true, 
        "contiguous_memory_optimization": false, 
        "number_checkpoints": null, 
        "synchronize_checkpoint_boundary": false, 
        "profile": false
    }
}
Using /home/xmw5190/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003592967987060547 seconds
DeepSpeedAgent Octavius
  0%|          | 0/23050 [00:00<?, ?it/s]/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/transformers/models/deit/feature_extraction_deit.py:28: FutureWarning: The class DeiTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DeiTImageProcessor instead.
  warnings.warn(

  0%|          | 0/10 [00:00<?, ?it/s][Ause prox
Epoch 1


Training on Image Data:   0%|          | 0/41 [00:00<?, ?it/s][A[A

Training on Image Data:   2%|â–         | 1/41 [00:09<06:13,  9.35s/it][A[A

Training on Image Data:   5%|â–         | 2/41 [00:13<03:57,  6.09s/it][A[A

Training on Image Data:   7%|â–‹         | 3/41 [00:16<03:10,  5.01s/it][A[A

Training on Image Data:  10%|â–‰         | 4/41 [00:20<02:46,  4.49s/it][A[A

Training on Image Data:  12%|â–ˆâ–        | 5/41 [00:24<02:29,  4.14s/it][A[A

Training on Image Data:  15%|â–ˆâ–        | 6/41 [00:27<02:18,  3.96s/it][A[A

Training on Image Data:  17%|â–ˆâ–‹        | 7/41 [00:31<02:09,  3.80s/it][A[A

Training on Image Data:  20%|â–ˆâ–‰        | 8/41 [00:34<02:05,  3.79s/it][A[A

Training on Image Data:  22%|â–ˆâ–ˆâ–       | 9/41 [00:38<02:01,  3.81s/it][A[A

Training on Image Data:  24%|â–ˆâ–ˆâ–       | 10/41 [00:42<01:55,  3.72s/it][A[A

Training on Image Data:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:45<01:49,  3.65s/it][A[A

Training on Image Data:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:49<01:42,  3.52s/it][A[A

Training on Image Data:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:52<01:37,  3.49s/it][A[A

Training on Image Data:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:56<01:37,  3.59s/it][A[A

Training on Image Data:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:59<01:31,  3.51s/it][A[A

Training on Image Data:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [01:03<01:28,  3.55s/it][A[A

Training on Image Data:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [01:06<01:26,  3.61s/it][A[A

Training on Image Data:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [01:10<01:20,  3.50s/it][A[A

Training on Image Data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [01:13<01:16,  3.49s/it][A[A

Training on Image Data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [01:17<01:12,  3.45s/it][A[A

Training on Image Data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [01:20<01:10,  3.52s/it][A[A

Training on Image Data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 22/41 [01:24<01:06,  3.50s/it][A[A

Training on Image Data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [01:27<01:02,  3.45s/it][A[A

Training on Image Data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [01:30<00:58,  3.42s/it][A[A

Training on Image Data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [01:34<00:54,  3.42s/it][A[A

Training on Image Data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 26/41 [01:37<00:51,  3.43s/it][A[A

Training on Image Data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [01:41<00:48,  3.44s/it][A[A

Training on Image Data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [01:44<00:45,  3.51s/it][A[A

Training on Image Data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [01:48<00:42,  3.51s/it][A[A

Training on Image Data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 30/41 [01:51<00:38,  3.49s/it][A[A

Training on Image Data:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [01:55<00:34,  3.46s/it][A[A

Training on Image Data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [01:58<00:30,  3.39s/it][A[A

Training on Image Data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [02:01<00:27,  3.40s/it][A[A

Training on Image Data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 34/41 [02:05<00:24,  3.49s/it][A[A

Training on Image Data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [02:09<00:21,  3.50s/it][A[A

Training on Image Data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [02:12<00:17,  3.46s/it][A[A

Training on Image Data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [02:15<00:13,  3.46s/it][A[A

Training on Image Data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 38/41 [02:19<00:10,  3.44s/it][A[A

Training on Image Data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [02:22<00:06,  3.42s/it][A[A

Training on Image Data:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [02:26<00:03,  3.48s/it][A[A

Training on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:27<00:00,  2.72s/it][A[ATraining on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:27<00:00,  3.59s/it]
Client Image Loss: 0.39517500978417514


Training on Image Data:   0%|          | 0/41 [00:00<?, ?it/s][A[A

Training on Image Data:   2%|â–         | 1/41 [00:03<02:22,  3.55s/it][A[A

Training on Image Data:   5%|â–         | 2/41 [00:07<02:23,  3.69s/it][A[A

Training on Image Data:   7%|â–‹         | 3/41 [00:10<02:13,  3.50s/it][A[A

Training on Image Data:  10%|â–‰         | 4/41 [00:13<02:03,  3.34s/it][A[A

Training on Image Data:  12%|â–ˆâ–        | 5/41 [00:17<02:00,  3.34s/it][A[A

Training on Image Data:  15%|â–ˆâ–        | 6/41 [00:20<02:01,  3.48s/it][A[A

Training on Image Data:  17%|â–ˆâ–‹        | 7/41 [00:24<01:55,  3.39s/it][A[A

Training on Image Data:  20%|â–ˆâ–‰        | 8/41 [00:27<01:49,  3.30s/it][A[A

Training on Image Data:  22%|â–ˆâ–ˆâ–       | 9/41 [00:30<01:46,  3.34s/it][A[A

Training on Image Data:  24%|â–ˆâ–ˆâ–       | 10/41 [00:33<01:41,  3.28s/it][A[A

Training on Image Data:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:36<01:37,  3.24s/it][A[A

Training on Image Data:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:40<01:34,  3.24s/it][A[A

Training on Image Data:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:43<01:33,  3.32s/it][A[A

Training on Image Data:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:47<01:32,  3.41s/it][A[A

Training on Image Data:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:50<01:26,  3.33s/it][A[A

Training on Image Data:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [00:53<01:23,  3.35s/it][A[A

Training on Image Data:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [00:56<01:19,  3.32s/it][A[A

Training on Image Data:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [01:00<01:16,  3.33s/it][A[A

Training on Image Data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [01:03<01:12,  3.30s/it][A[A

Training on Image Data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [01:07<01:10,  3.38s/it][A[A

Training on Image Data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [01:10<01:04,  3.23s/it][A[A

Training on Image Data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 22/41 [01:13<01:03,  3.34s/it][A[A

Training on Image Data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [01:16<00:59,  3.33s/it][A[A

Training on Image Data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [01:20<00:56,  3.34s/it][A[A

Training on Image Data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [01:23<00:52,  3.26s/it][A[A

Training on Image Data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 26/41 [01:26<00:49,  3.28s/it][A[A

Training on Image Data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [01:29<00:45,  3.24s/it][A[A

Training on Image Data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [01:32<00:41,  3.16s/it][A[A

Training on Image Data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [01:35<00:37,  3.14s/it][A[A

Training on Image Data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 30/41 [01:39<00:35,  3.21s/it][A[A

Training on Image Data:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [01:42<00:32,  3.23s/it][A[A

Training on Image Data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [01:45<00:28,  3.20s/it][A[A

Training on Image Data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [01:48<00:25,  3.18s/it][A[A

Training on Image Data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 34/41 [01:51<00:21,  3.09s/it][A[A

Training on Image Data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [01:54<00:18,  3.14s/it][A[A

Training on Image Data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [01:58<00:15,  3.12s/it][A[A

Training on Image Data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [02:01<00:12,  3.14s/it][A[A

Training on Image Data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 38/41 [02:04<00:09,  3.17s/it][A[A

Training on Image Data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [02:07<00:06,  3.05s/it][A[A

Training on Image Data:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [02:10<00:03,  3.01s/it][A[A

Training on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:10<00:00,  2.34s/it][A[ATraining on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:10<00:00,  3.19s/it]
Client Image Loss: 0.33128644489660497


Training on Image Data:   0%|          | 0/41 [00:00<?, ?it/s][A[A

Training on Image Data:   2%|â–         | 1/41 [00:02<01:55,  2.89s/it][A[A

Training on Image Data:   5%|â–         | 2/41 [00:06<02:02,  3.14s/it][A[A

Training on Image Data:   7%|â–‹         | 3/41 [00:09<01:58,  3.11s/it][A[A

Training on Image Data:  10%|â–‰         | 4/41 [00:12<01:53,  3.06s/it][A[A

Training on Image Data:  12%|â–ˆâ–        | 5/41 [00:15<01:58,  3.30s/it][A[A

Training on Image Data:  15%|â–ˆâ–        | 6/41 [00:19<01:53,  3.25s/it][A[A

Training on Image Data:  17%|â–ˆâ–‹        | 7/41 [00:22<01:51,  3.29s/it][A[A

Training on Image Data:  20%|â–ˆâ–‰        | 8/41 [00:25<01:50,  3.34s/it][A[A

Training on Image Data:  22%|â–ˆâ–ˆâ–       | 9/41 [00:29<01:44,  3.27s/it][A[A

Training on Image Data:  24%|â–ˆâ–ˆâ–       | 10/41 [00:32<01:43,  3.35s/it][A[A

Training on Image Data:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:35<01:38,  3.29s/it][A[A

Training on Image Data:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:39<01:35,  3.28s/it][A[A

Training on Image Data:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:42<01:33,  3.34s/it][A[A

Training on Image Data:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:45<01:28,  3.29s/it][A[A

Training on Image Data:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:48<01:24,  3.26s/it][A[A

Training on Image Data:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [00:52<01:22,  3.32s/it][A[A

Training on Image Data:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [00:55<01:17,  3.24s/it][A[A

Training on Image Data:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [00:58<01:13,  3.19s/it][A[A

Training on Image Data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [01:01<01:10,  3.22s/it][A[A

Training on Image Data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [01:05<01:08,  3.26s/it][A[A

Training on Image Data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [01:08<01:06,  3.33s/it][A[A

Training on Image Data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 22/41 [01:11<01:00,  3.19s/it][A[A

Training on Image Data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [01:14<00:58,  3.25s/it][A[A

Training on Image Data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [01:18<00:55,  3.29s/it][A[A

Training on Image Data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [01:21<00:53,  3.36s/it][A[A

Training on Image Data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 26/41 [01:24<00:49,  3.30s/it][A[A

Training on Image Data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [01:27<00:45,  3.22s/it][A[A

Training on Image Data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [01:30<00:40,  3.15s/it][A[A

Training on Image Data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [01:34<00:38,  3.19s/it][A[A

Training on Image Data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 30/41 [01:37<00:36,  3.31s/it][A[A

Training on Image Data:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [01:41<00:33,  3.39s/it][A[A

Training on Image Data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [01:44<00:30,  3.35s/it][A[A

Training on Image Data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [01:47<00:26,  3.25s/it][A[A

Training on Image Data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 34/41 [01:50<00:22,  3.20s/it][A[A

Training on Image Data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [01:54<00:19,  3.24s/it][A[A

Training on Image Data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [01:57<00:16,  3.38s/it][A[A

Training on Image Data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [02:01<00:14,  3.51s/it][A[A

Training on Image Data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 38/41 [02:04<00:10,  3.44s/it][A[A

Training on Image Data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [02:08<00:06,  3.44s/it][A[A

Training on Image Data:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [02:11<00:03,  3.41s/it][A[A

Training on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:12<00:00,  2.68s/it][A[ATraining on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:12<00:00,  3.24s/it]
Client Image Loss: 0.33953571028825713


Training on Image Data:   0%|          | 0/41 [00:00<?, ?it/s][A[A

Training on Image Data:   2%|â–         | 1/41 [00:03<02:17,  3.43s/it][A[A

Training on Image Data:   5%|â–         | 2/41 [00:06<02:10,  3.36s/it][A[A

Training on Image Data:   7%|â–‹         | 3/41 [00:10<02:12,  3.50s/it][A[A

Training on Image Data:  10%|â–‰         | 4/41 [00:14<02:11,  3.54s/it][A[A

Training on Image Data:  12%|â–ˆâ–        | 5/41 [00:17<02:04,  3.45s/it][A[A

Training on Image Data:  15%|â–ˆâ–        | 6/41 [00:20<01:58,  3.39s/it][A[A

Training on Image Data:  17%|â–ˆâ–‹        | 7/41 [00:24<01:55,  3.41s/it][A[A

Training on Image Data:  20%|â–ˆâ–‰        | 8/41 [00:28<02:03,  3.75s/it][A[A

Training on Image Data:  22%|â–ˆâ–ˆâ–       | 9/41 [00:31<01:54,  3.58s/it][A[A

Training on Image Data:  24%|â–ˆâ–ˆâ–       | 10/41 [00:34<01:46,  3.43s/it][A[A

Training on Image Data:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:38<01:44,  3.47s/it][A[A

Training on Image Data:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:41<01:39,  3.44s/it][A[A

Training on Image Data:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:45<01:37,  3.48s/it][A[A

Training on Image Data:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:49<01:43,  3.84s/it][A[A

Training on Image Data:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:54<01:44,  4.03s/it][A[A

Training on Image Data:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [00:59<01:44,  4.19s/it][A[A

Training on Image Data:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [01:02<01:36,  4.00s/it][A[A

Training on Image Data:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [01:06<01:31,  3.99s/it][A[A

Training on Image Data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [01:10<01:25,  3.87s/it][A[A

Training on Image Data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [01:13<01:20,  3.81s/it][A[A

Training on Image Data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [01:17<01:15,  3.77s/it][A[A

Training on Image Data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 22/41 [01:20<01:09,  3.64s/it][A[A

Training on Image Data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [01:24<01:07,  3.75s/it][A[A

Training on Image Data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [01:28<01:01,  3.64s/it][A[A

Training on Image Data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [01:31<00:58,  3.64s/it][A[A

Training on Image Data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 26/41 [01:35<00:56,  3.77s/it][A[A

Training on Image Data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [01:39<00:52,  3.75s/it][A[A

Training on Image Data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [01:43<00:48,  3.72s/it][A[A

Training on Image Data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [01:46<00:43,  3.67s/it][A[A

Training on Image Data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 30/41 [01:50<00:41,  3.74s/it][A[A

Training on Image Data:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [01:54<00:37,  3.72s/it][A[A

Training on Image Data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [01:58<00:33,  3.70s/it][A[A

Training on Image Data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [02:02<00:30,  3.82s/it][A[A

Training on Image Data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 34/41 [02:05<00:25,  3.71s/it][A[A

Training on Image Data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [02:09<00:22,  3.80s/it][A[A

Training on Image Data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [02:13<00:19,  3.85s/it][A[A

Training on Image Data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [02:17<00:15,  3.88s/it][A[A

Training on Image Data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 38/41 [02:20<00:11,  3.74s/it][A[A

Training on Image Data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [02:24<00:07,  3.68s/it][A[A

Training on Image Data:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [02:27<00:03,  3.59s/it][A[A

Training on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:28<00:00,  2.77s/it][A[ATraining on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:28<00:00,  3.63s/it]
Client Image Loss: 0.3748232660497107


Training on Image Data:   0%|          | 0/41 [00:00<?, ?it/s][A[A

Training on Image Data:   2%|â–         | 1/41 [00:03<02:25,  3.64s/it][A[A

Training on Image Data:   5%|â–         | 2/41 [00:07<02:31,  3.87s/it][A[A

Training on Image Data:   7%|â–‹         | 3/41 [00:11<02:26,  3.86s/it][A[A

Training on Image Data:  10%|â–‰         | 4/41 [00:15<02:23,  3.89s/it][A[A

Training on Image Data:  12%|â–ˆâ–        | 5/41 [00:18<02:09,  3.59s/it][A[A

Training on Image Data:  15%|â–ˆâ–        | 6/41 [00:21<02:03,  3.52s/it][A[A

Training on Image Data:  17%|â–ˆâ–‹        | 7/41 [00:25<01:59,  3.50s/it][A[A

Training on Image Data:  20%|â–ˆâ–‰        | 8/41 [00:29<02:00,  3.65s/it][A[A

Training on Image Data:  22%|â–ˆâ–ˆâ–       | 9/41 [00:32<01:55,  3.62s/it][A[A

Training on Image Data:  24%|â–ˆâ–ˆâ–       | 10/41 [00:36<01:51,  3.61s/it][A[A

Training on Image Data:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:40<01:47,  3.60s/it][A[A

Training on Image Data:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:43<01:40,  3.46s/it][A[A

Training on Image Data:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:46<01:37,  3.48s/it][A[A

Training on Image Data:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:50<01:36,  3.59s/it][A[A

Training on Image Data:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:56<01:53,  4.38s/it][A[A

Training on Image Data:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [01:00<01:41,  4.07s/it][A[A

Training on Image Data:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [01:03<01:34,  3.96s/it][A[A

Training on Image Data:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [01:07<01:28,  3.85s/it][A[A

Training on Image Data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [01:10<01:22,  3.75s/it][A[A

Training on Image Data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [01:14<01:18,  3.72s/it][A[A

Training on Image Data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [01:18<01:12,  3.64s/it][A[A

Training on Image Data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 22/41 [01:21<01:08,  3.59s/it][A[A

Training on Image Data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [01:24<01:03,  3.55s/it][A[A

Training on Image Data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [01:29<01:03,  3.76s/it][A[A

Training on Image Data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [01:32<00:58,  3.63s/it][A[A

Training on Image Data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 26/41 [01:36<00:54,  3.63s/it][A[A

Training on Image Data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [01:39<00:51,  3.65s/it][A[A

Training on Image Data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [01:43<00:46,  3.60s/it][A[A

Training on Image Data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [01:46<00:42,  3.53s/it][A[A

Training on Image Data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 30/41 [01:50<00:39,  3.58s/it][A[A

Training on Image Data:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [01:53<00:34,  3.48s/it][A[A

Training on Image Data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [01:56<00:30,  3.40s/it][A[A

Training on Image Data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [02:00<00:27,  3.42s/it][A[A

Training on Image Data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 34/41 [02:04<00:24,  3.53s/it][A[A

Training on Image Data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [02:07<00:20,  3.42s/it][A[A

Training on Image Data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [02:10<00:16,  3.32s/it][A[A

Training on Image Data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [02:13<00:13,  3.40s/it][A[A

Training on Image Data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 38/41 [02:17<00:10,  3.39s/it][A[A

Training on Image Data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [02:20<00:06,  3.42s/it][A[A

Training on Image Data:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [02:24<00:03,  3.45s/it][A[A

Training on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:25<00:00,  2.68s/it][A[ATraining on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:25<00:00,  3.54s/it]
Client Image Loss: 0.36807283541051355
  0%|          | 0/10 [11:53<?, ?it/s]
Traceback (most recent call last):
  File "/home/xmw5190/FedMFM/LAMM/src/train.py", line 499, in <module>
    main(**cfg)
  File "/home/xmw5190/FedMFM/LAMM/src/train.py", line 447, in main
    agent.train_model(batch, current_step=current_step, pbar=pbar)
  File "/home/xmw5190/FedMFM/LAMM/src/model/training_agent.py", line 41, in train_model
    loss, mle_acc = self.ds_engine(batch)
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1736, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xmw5190/FedMFM/LAMM/src/model/Octavius/octavius.py", line 573, in forward
    self.moe_set_Xattn_gate(
  File "/home/xmw5190/FedMFM/LAMM/src/model/Octavius/octavius.py", line 518, in moe_set_Xattn_gate
    input_embeds_0 = embedding_layer(input_tokens_0)
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 158, in forward
    return F.embedding(
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torch/nn/functional.py", line 2199, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument index in method wrapper__index_select)
  0%|          | 0/23050 [11:53<?, ?it/s]
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2578730) of binary: /home/xmw5190/.conda/envs/lamm/bin/python
Traceback (most recent call last):
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-06_08:22:55
  host      : i4-l-ffm5105-02.ad.psu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2578730)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
