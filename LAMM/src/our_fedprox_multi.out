nohup: ignoring input
/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Setting ds_accelerator to cuda (auto detect)
/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:25: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
WARNING:root:Could not import _ext module.
Please see the setup instructions in the README: https://github.com/erikwijmans/Pointnet2_PyTorch/blob/master/README.rst. Please refer to README.md to install optional extension for 3D environment if required.
WARNING:root:No module named 'lightllm'. Please refer to README.md to install optional LightLLM extension if required.
Arguments: 
{
    "cfg": "config/Octavius/octavius_2d_e4_bs64.yaml",
    "conv_template": "default",
    "data_path": null,
    "delta_ckpt_path": null,
    "encoder_ckpt_path": null,
    "encoder_pretrain": "clip",
    "gradient_checkpointing": false,
    "llm_ckpt_path": "/data/xiaochen/FedMFM/MMedLM2/",
    "llm_proj_path": null,
    "local_rank": 0,
    "log_path": "../ckpt/octavius_2d_e4_bs64/log_rest/",
    "max_tgt_len": 128,
    "model": "octavius",
    "num_clients": "5",
    "num_points": 40000,
    "num_vision_token": 198,
    "save_path": "/data/xiaochen/FedMFM/ckpt/ours_fedprox_multi",
    "stage": 1,
    "use_color": false,
    "use_flash_attn": false,
    "use_height": false,
    "use_system": true,
    "use_xformers": false,
    "vision_feature_type": "local",
    "vision_output_layer": -1,
    "vision_root_path": null,
    "vision_type": "image"
}
[2024-06-05 12:39:57,118] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-06-05 12:39:57,118] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-06-05 12:39:57,118] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Initializing [clip] visual encoder from ~/.cache/clip/ViT-L-14.pt [cuda]...
/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/transformers/models/deit/feature_extraction_deit.py:28: FutureWarning: The class DeiTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DeiTImageProcessor instead.
  warnings.warn(
Visual encoder initialized.
Initializing language decoder from /data/xiaochen/FedMFM/MMedLM2/ ...
Build PEFT model with LoRA-MoE.
loading medical ckpt
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:06<00:20,  6.67s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:12<00:12,  6.42s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:20<00:00,  4.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:20<00:00,  5.07s/it]
InternLM2ForCausalLM(
  (model): InternLM2Model(
    (tok_embeddings): Embedding(92544, 4096, padding_idx=2)
    (layers): ModuleList(
      (0): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (1): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (2): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (3): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (4): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (5): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (6): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (7): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (8): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (9): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (10): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (11): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (12): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (13): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (14): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (15): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (16): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (17): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (18): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (19): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (20): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (21): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (22): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (23): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (24): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (25): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (26): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (27): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (28): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (29): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (30): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
      (31): InternLM2DecoderLayer(
        (attention): InternLM2Attention(
          (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): InternLM2RotaryEmbedding()
        )
        (feed_forward): InternLM2MLP(
          (w1): Linear(in_features=4096, out_features=14336, bias=False)
          (w3): Linear(in_features=4096, out_features=14336, bias=False)
          (w2): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (attention_norm): InternLM2RMSNorm()
        (ffn_norm): InternLM2RMSNorm()
      )
    )
    (norm): InternLM2RMSNorm()
  )
  (output): Linear(in_features=4096, out_features=92544, bias=False)
)
trainable params: 78589952 || all params: 7816298496 || trainable%: 1.0054625221928064
Language decoder initialized.
Add VISION TAG ("<Img>" and "</Img>") for modality image.
Octavius Modalities: ['image']
Octavius 2D projection layer initialized.
Signal processing module initialized.
Clinical readings processing module initialized.
Demographics MLP encoder initialized.
[2024-06-05 12:42:59,937] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.3, git-hash=4e80e29, git-branch=HEAD
[2024-06-05 12:42:59,937] [INFO] [comm.py:619:init_distributed] Distributed backend already initialized
[2024-06-05 12:43:13,504] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/xmw5190/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Emitting ninja build file /home/xmw5190/.cache/torch_extensions/py310_cu113/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.501147985458374 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000500, betas=(0.900000, 0.950000), weight_decay=0.000010, adam_w=1
[2024-06-05 12:43:17,595] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-06-05 12:43:17,701] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-06-05 12:43:17,702] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-06-05 12:43:17,702] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
[2024-06-05 12:43:17,702] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2024-06-05 12:43:17,702] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2024-06-05 12:43:17,702] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2024-06-05 12:43:17,702] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/xmw5190/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Emitting ninja build file /home/xmw5190/.cache/torch_extensions/py310_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.12906384468078613 seconds
Rank: 0 partition count [1] and sizes[(892644780, False)] 
[2024-06-05 12:43:23,561] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2024-06-05 12:43:23,562] [INFO] [utils.py:786:see_memory_usage] MA 15.88 GB         Max_MA 15.88 GB         CA 16.02 GB         Max_CA 16 GB 
[2024-06-05 12:43:23,562] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 37.65 GB, percent = 10.0%
[2024-06-05 12:43:26,630] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2024-06-05 12:43:26,630] [INFO] [utils.py:786:see_memory_usage] MA 15.88 GB         Max_MA 15.88 GB         CA 16.02 GB         Max_CA 16 GB 
[2024-06-05 12:43:26,630] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 48.42 GB, percent = 12.9%
[2024-06-05 12:43:26,631] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2024-06-05 12:43:26,735] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2024-06-05 12:43:26,736] [INFO] [utils.py:786:see_memory_usage] MA 15.88 GB         Max_MA 15.88 GB         CA 16.02 GB         Max_CA 16 GB 
[2024-06-05 12:43:26,736] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 48.42 GB, percent = 12.9%
[2024-06-05 12:43:26,745] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2024-06-05 12:43:26,745] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-06-05 12:43:26,745] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7ff7edb93fd0>
[2024-06-05 12:43:26,745] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0005], mom=[[0.9, 0.95]]
[2024-06-05 12:43:26,749] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": true, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": true, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   amp_enabled .................. False
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   amp_params ................... False
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff81c236050>
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   communication_data_type ...... None
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-05 12:43:26,749] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   disable_allgather ............ False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   dump_state ................... False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 128}
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   global_rank .................. 0
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 8
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   gradient_clipping ............ 1
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 65536
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   loss_scale ................... 0
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   optimizer_params ............. {'betas': [0.9, 0.95], 'eps': 1e-06, 'lr': 0.0005, 'weight_decay': 1e-05}
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   pld_enabled .................. False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   pld_params ................... False
[2024-06-05 12:43:26,750] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 1440, 'warmup_max_lr': 0.0001, 'warmup_min_lr': 0, 'warmup_num_steps': 144}
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   sparse_attention ............. None
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   steps_per_print .............. 1
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   train_batch_size ............. 64
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  8
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   world_size ................... 1
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   zero_enabled ................. True
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-05 12:43:26,751] [INFO] [config.py:964:print]   zero_optimization_stage ...... 1
[2024-06-05 12:43:26,751] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 8, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1, 
    "steps_per_print": 1, 
    "zero_optimization": {
        "allgather_bucket_size": 5.000000e+08, 
        "allgather_partitions": true, 
        "contiguous_gradients": true, 
        "offload_optimizer": {
            "device": "cpu"
        }, 
        "stage": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "betas": [0.9, 0.95], 
            "eps": 1e-06, 
            "lr": 0.0005, 
            "weight_decay": 1e-05
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "total_num_steps": 1.440000e+03, 
            "warmup_max_lr": 0.0001, 
            "warmup_min_lr": 0, 
            "warmup_num_steps": 144
        }
    }, 
    "fp16": {
        "enabled": true, 
        "min_loss_scale": 128, 
        "opt_level": "O2"
    }, 
    "bf16": {
        "enable": false
    }, 
    "activation_checkpointing": {
        "partition_activations": true, 
        "cpu_checkpointing": true, 
        "contiguous_memory_optimization": false, 
        "number_checkpoints": null, 
        "synchronize_checkpoint_boundary": false, 
        "profile": false
    }
}
Using /home/xmw5190/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00036215782165527344 seconds
DeepSpeedAgent Octavius
  0%|          | 0/11525 [00:00<?, ?it/s]/home/xmw5190/.conda/envs/lamm/lib/python3.10/site-packages/transformers/models/deit/feature_extraction_deit.py:28: FutureWarning: The class DeiTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DeiTImageProcessor instead.
  warnings.warn(

  0%|          | 0/10 [00:00<?, ?it/s][Ause prox
Epoch 1


Training on Image Data:   0%|          | 0/41 [00:00<?, ?it/s][A[A

Training on Image Data:   2%|â–         | 1/41 [00:05<03:24,  5.12s/it][A[A

Training on Image Data:   5%|â–         | 2/41 [00:07<02:20,  3.62s/it][A[A

Training on Image Data:   7%|â–‹         | 3/41 [00:10<01:56,  3.07s/it][A[A

Training on Image Data:  10%|â–‰         | 4/41 [00:12<01:42,  2.76s/it][A[A

Training on Image Data:  12%|â–ˆâ–        | 5/41 [00:14<01:33,  2.59s/it][A[A

Training on Image Data:  15%|â–ˆâ–        | 6/41 [00:16<01:25,  2.45s/it][A[A

Training on Image Data:  17%|â–ˆâ–‹        | 7/41 [00:19<01:20,  2.36s/it][A[A

Training on Image Data:  20%|â–ˆâ–‰        | 8/41 [00:21<01:19,  2.41s/it][A[A

Training on Image Data:  22%|â–ˆâ–ˆâ–       | 9/41 [00:23<01:14,  2.31s/it][A[A

Training on Image Data:  24%|â–ˆâ–ˆâ–       | 10/41 [00:25<01:09,  2.24s/it][A[A

Training on Image Data:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:27<01:07,  2.25s/it][A[A

Training on Image Data:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:30<01:09,  2.41s/it][A[A

Training on Image Data:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:33<01:06,  2.37s/it][A[A

Training on Image Data:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:35<01:03,  2.36s/it][A[A

Training on Image Data:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:37<01:00,  2.32s/it][A[A

Training on Image Data:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [00:40<00:59,  2.39s/it][A[A

Training on Image Data:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [00:42<00:56,  2.35s/it][A[A

Training on Image Data:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [00:44<00:53,  2.34s/it][A[A

Training on Image Data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [00:46<00:50,  2.28s/it][A[A

Training on Image Data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [00:49<00:49,  2.34s/it][A[A

Training on Image Data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [00:51<00:46,  2.32s/it][A[A

Training on Image Data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 22/41 [00:53<00:43,  2.27s/it][A[A

Training on Image Data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [00:55<00:40,  2.24s/it][A[A

Training on Image Data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [00:58<00:39,  2.35s/it][A[A

Training on Image Data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [01:00<00:37,  2.31s/it][A[A

Training on Image Data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 26/41 [01:03<00:34,  2.28s/it][A[A

Training on Image Data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [01:05<00:31,  2.25s/it][A[A

Training on Image Data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [01:07<00:29,  2.28s/it][A[A

Training on Image Data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [01:09<00:27,  2.30s/it][A[A

Training on Image Data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 30/41 [01:12<00:25,  2.29s/it][A[A

Training on Image Data:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [01:14<00:22,  2.27s/it][A[A

Training on Image Data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [01:16<00:20,  2.32s/it][A[A

Training on Image Data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [01:19<00:18,  2.31s/it][A[A

Training on Image Data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 34/41 [01:21<00:15,  2.22s/it][A[A

Training on Image Data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [01:23<00:13,  2.19s/it][A[A

Training on Image Data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [01:25<00:10,  2.16s/it][A[A

Training on Image Data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [01:27<00:08,  2.25s/it][A[A

Training on Image Data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 38/41 [01:30<00:06,  2.27s/it][A[A

Training on Image Data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [01:32<00:04,  2.30s/it][A[A

Training on Image Data:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [01:34<00:02,  2.26s/it][A[A

Training on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:35<00:00,  1.76s/it][A[ATraining on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:35<00:00,  2.32s/it]
Client Image Loss: 0.33894182596264816


Training on Image Data:   0%|          | 0/41 [00:00<?, ?it/s][A[A

Training on Image Data:   2%|â–         | 1/41 [00:02<01:23,  2.08s/it][A[A

Training on Image Data:   5%|â–         | 2/41 [00:04<01:30,  2.33s/it][A[A

Training on Image Data:   7%|â–‹         | 3/41 [00:07<01:32,  2.44s/it][A[A

Training on Image Data:  10%|â–‰         | 4/41 [00:09<01:26,  2.34s/it][A[A

Training on Image Data:  12%|â–ˆâ–        | 5/41 [00:11<01:20,  2.24s/it][A[A

Training on Image Data:  15%|â–ˆâ–        | 6/41 [00:13<01:15,  2.15s/it][A[A

Training on Image Data:  17%|â–ˆâ–‹        | 7/41 [00:15<01:12,  2.14s/it][A[A

Training on Image Data:  20%|â–ˆâ–‰        | 8/41 [00:17<01:10,  2.15s/it][A[A

Training on Image Data:  22%|â–ˆâ–ˆâ–       | 9/41 [00:19<01:09,  2.18s/it][A[A

Training on Image Data:  24%|â–ˆâ–ˆâ–       | 10/41 [00:22<01:07,  2.19s/it][A[A

Training on Image Data:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:24<01:06,  2.22s/it][A[A

Training on Image Data:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:26<01:03,  2.17s/it][A[A

Training on Image Data:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:28<00:59,  2.13s/it][A[A

Training on Image Data:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:30<00:57,  2.11s/it][A[A

Training on Image Data:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:32<00:54,  2.10s/it][A[A

Training on Image Data:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [00:34<00:52,  2.10s/it][A[A

Training on Image Data:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [00:36<00:50,  2.09s/it][A[A

Training on Image Data:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [00:38<00:47,  2.05s/it][A[A

Training on Image Data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [00:40<00:45,  2.08s/it][A[A

Training on Image Data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [00:43<00:45,  2.17s/it][A[A

Training on Image Data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [00:45<00:42,  2.15s/it][A[A

Training on Image Data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 22/41 [00:47<00:40,  2.11s/it][A[A

Training on Image Data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [00:49<00:37,  2.09s/it][A[A

Training on Image Data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [00:52<00:37,  2.23s/it][A[A

Training on Image Data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [00:54<00:36,  2.28s/it][A[A

Training on Image Data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 26/41 [00:56<00:33,  2.23s/it][A[A

Training on Image Data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [00:58<00:30,  2.16s/it][A[A

Training on Image Data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [01:00<00:28,  2.19s/it][A[A

Training on Image Data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [01:02<00:25,  2.14s/it][A[A

Training on Image Data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 30/41 [01:04<00:23,  2.13s/it][A[A

Training on Image Data:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [01:06<00:20,  2.09s/it][A[A

Training on Image Data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [01:09<00:19,  2.12s/it][A[A

Training on Image Data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [01:11<00:16,  2.11s/it][A[A

Training on Image Data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 34/41 [01:13<00:14,  2.07s/it][A[A

Training on Image Data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [01:15<00:12,  2.09s/it][A[A

Training on Image Data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [01:17<00:10,  2.16s/it][A[A

Training on Image Data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [01:19<00:08,  2.22s/it][A[A

Training on Image Data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 38/41 [01:22<00:06,  2.19s/it][A[A

Training on Image Data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [01:24<00:04,  2.19s/it][A[A

Training on Image Data:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [01:26<00:02,  2.18s/it][A[A

Training on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:27<00:00,  1.72s/it][A[ATraining on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:27<00:00,  2.12s/it]
Client Image Loss: 0.3603775744758001


Training on Image Data:   0%|          | 0/41 [00:00<?, ?it/s][A[A

Training on Image Data:   2%|â–         | 1/41 [00:02<01:20,  2.00s/it][A[A

Training on Image Data:   5%|â–         | 2/41 [00:04<01:25,  2.18s/it][A[A

Training on Image Data:   7%|â–‹         | 3/41 [00:06<01:24,  2.22s/it][A[A

Training on Image Data:  10%|â–‰         | 4/41 [00:08<01:23,  2.26s/it][A[A

Training on Image Data:  12%|â–ˆâ–        | 5/41 [00:11<01:20,  2.23s/it][A[A

Training on Image Data:  15%|â–ˆâ–        | 6/41 [00:13<01:17,  2.23s/it][A[A

Training on Image Data:  17%|â–ˆâ–‹        | 7/41 [00:15<01:13,  2.17s/it][A[A

Training on Image Data:  20%|â–ˆâ–‰        | 8/41 [00:17<01:12,  2.20s/it][A[A

Training on Image Data:  22%|â–ˆâ–ˆâ–       | 9/41 [00:19<01:10,  2.21s/it][A[A

Training on Image Data:  24%|â–ˆâ–ˆâ–       | 10/41 [00:21<01:07,  2.17s/it][A[A

Training on Image Data:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:23<01:04,  2.14s/it][A[A

Training on Image Data:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:26<01:01,  2.13s/it][A[A

Training on Image Data:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:28<00:58,  2.07s/it][A[A

Training on Image Data:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:30<00:55,  2.05s/it][A[A

Training on Image Data:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:32<00:52,  2.03s/it][A[A

Training on Image Data:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [00:33<00:50,  2.01s/it][A[A

Training on Image Data:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [00:36<00:48,  2.03s/it][A[A

Training on Image Data:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [00:38<00:48,  2.13s/it][A[A

Training on Image Data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [00:40<00:48,  2.18s/it][A[A

Training on Image Data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [00:42<00:45,  2.18s/it][A[A

Training on Image Data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [00:45<00:43,  2.19s/it][A[A

Training on Image Data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 22/41 [00:47<00:42,  2.24s/it][A[A

Training on Image Data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [00:49<00:39,  2.22s/it][A[A

Training on Image Data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [00:51<00:38,  2.25s/it][A[A

Training on Image Data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [00:54<00:36,  2.27s/it][A[A

Training on Image Data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 26/41 [00:56<00:33,  2.21s/it][A[A

Training on Image Data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [00:58<00:30,  2.19s/it][A[A

Training on Image Data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [01:00<00:29,  2.27s/it][A[A

Training on Image Data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [01:03<00:28,  2.36s/it][A[A

Training on Image Data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 30/41 [01:07<00:30,  2.77s/it][A[A

Training on Image Data:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [01:09<00:27,  2.72s/it][A[A

Training on Image Data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [01:11<00:22,  2.54s/it][A[A

Training on Image Data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [01:14<00:19,  2.41s/it][A[A

Training on Image Data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 34/41 [01:16<00:16,  2.37s/it][A[A

Training on Image Data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [01:18<00:13,  2.30s/it][A[A

Training on Image Data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [01:20<00:11,  2.29s/it][A[A

Training on Image Data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [01:22<00:09,  2.27s/it][A[A

Training on Image Data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 38/41 [01:25<00:06,  2.30s/it][A[A

Training on Image Data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [01:27<00:04,  2.27s/it][A[A

Training on Image Data:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [01:29<00:02,  2.28s/it][A[A

Training on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:30<00:00,  1.78s/it][A[ATraining on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:30<00:00,  2.21s/it]
Client Image Loss: 0.3460766449570656


Training on Image Data:   0%|          | 0/41 [00:00<?, ?it/s][A[A

Training on Image Data:   2%|â–         | 1/41 [00:02<01:41,  2.54s/it][A[A

Training on Image Data:   5%|â–         | 2/41 [00:05<01:41,  2.60s/it][A[A

Training on Image Data:   7%|â–‹         | 3/41 [00:07<01:34,  2.50s/it][A[A

Training on Image Data:  10%|â–‰         | 4/41 [00:09<01:30,  2.46s/it][A[A

Training on Image Data:  12%|â–ˆâ–        | 5/41 [00:12<01:28,  2.46s/it][A[A

Training on Image Data:  15%|â–ˆâ–        | 6/41 [00:14<01:23,  2.39s/it][A[A

Training on Image Data:  17%|â–ˆâ–‹        | 7/41 [00:16<01:19,  2.35s/it][A[A

Training on Image Data:  20%|â–ˆâ–‰        | 8/41 [00:19<01:16,  2.32s/it][A[A

Training on Image Data:  22%|â–ˆâ–ˆâ–       | 9/41 [00:21<01:12,  2.26s/it][A[A

Training on Image Data:  24%|â–ˆâ–ˆâ–       | 10/41 [00:23<01:09,  2.26s/it][A[A

Training on Image Data:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:25<01:06,  2.22s/it][A[A

Training on Image Data:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:27<01:04,  2.23s/it][A[A

Training on Image Data:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:30<01:01,  2.20s/it][A[A

Training on Image Data:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:32<00:58,  2.18s/it][A[A

Training on Image Data:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:34<00:57,  2.22s/it][A[A

Training on Image Data:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [00:36<00:54,  2.19s/it][A[A

Training on Image Data:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [00:38<00:52,  2.18s/it][A[A

Training on Image Data:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [00:40<00:50,  2.18s/it][A[A

Training on Image Data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [00:43<00:48,  2.19s/it][A[A

Training on Image Data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [00:45<00:46,  2.22s/it][A[A

Training on Image Data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [00:47<00:45,  2.26s/it][A[A

Training on Image Data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 22/41 [00:50<00:42,  2.26s/it][A[A

Training on Image Data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [00:52<00:39,  2.21s/it][A[A

Training on Image Data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [00:54<00:37,  2.22s/it][A[A

Training on Image Data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [00:56<00:34,  2.18s/it][A[A

Training on Image Data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 26/41 [00:58<00:32,  2.14s/it][A[A

Training on Image Data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [01:01<00:32,  2.30s/it][A[A

Training on Image Data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [01:04<00:31,  2.45s/it][A[A

Training on Image Data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [01:06<00:28,  2.38s/it][A[A

Training on Image Data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 30/41 [01:08<00:25,  2.30s/it][A[A

Training on Image Data:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [01:10<00:22,  2.21s/it][A[A

Training on Image Data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [01:12<00:20,  2.24s/it][A[A

Training on Image Data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [01:14<00:17,  2.17s/it][A[A

Training on Image Data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 34/41 [01:16<00:14,  2.12s/it][A[A

Training on Image Data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [01:18<00:12,  2.14s/it][A[A

Training on Image Data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [01:21<00:11,  2.28s/it][A[A

Training on Image Data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [01:23<00:09,  2.29s/it][A[A

Training on Image Data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 38/41 [01:25<00:06,  2.23s/it][A[A

Training on Image Data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [01:28<00:04,  2.22s/it][A[A

Training on Image Data:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [01:30<00:02,  2.23s/it][A[A

Training on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:30<00:00,  1.73s/it][A[ATraining on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:30<00:00,  2.22s/it]
Client Image Loss: 0.33354210194836303


Training on Image Data:   0%|          | 0/41 [00:00<?, ?it/s][A[A

Training on Image Data:   2%|â–         | 1/41 [00:02<01:20,  2.02s/it][A[A

Training on Image Data:   5%|â–         | 2/41 [00:04<01:19,  2.03s/it][A[A

Training on Image Data:   7%|â–‹         | 3/41 [00:06<01:20,  2.11s/it][A[A

Training on Image Data:  10%|â–‰         | 4/41 [00:08<01:19,  2.14s/it][A[A

Training on Image Data:  12%|â–ˆâ–        | 5/41 [00:10<01:17,  2.15s/it][A[A

Training on Image Data:  15%|â–ˆâ–        | 6/41 [00:12<01:16,  2.18s/it][A[A

Training on Image Data:  17%|â–ˆâ–‹        | 7/41 [00:15<01:14,  2.19s/it][A[A

Training on Image Data:  20%|â–ˆâ–‰        | 8/41 [00:17<01:14,  2.25s/it][A[A

Training on Image Data:  22%|â–ˆâ–ˆâ–       | 9/41 [00:19<01:09,  2.18s/it][A[A

Training on Image Data:  24%|â–ˆâ–ˆâ–       | 10/41 [00:21<01:06,  2.15s/it][A[A

Training on Image Data:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:23<01:05,  2.18s/it][A[A

Training on Image Data:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:25<01:03,  2.18s/it][A[A

Training on Image Data:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:28<01:00,  2.15s/it][A[A

Training on Image Data:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:30<00:57,  2.14s/it][A[A

Training on Image Data:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:32<00:55,  2.15s/it][A[A

Training on Image Data:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [00:34<00:54,  2.19s/it][A[A

Training on Image Data:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [00:36<00:51,  2.16s/it][A[A

Training on Image Data:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [00:38<00:50,  2.18s/it][A[A

Training on Image Data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [00:41<00:48,  2.19s/it][A[A

Training on Image Data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [00:43<00:45,  2.16s/it][A[A

Training on Image Data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [00:45<00:42,  2.12s/it][A[A

Training on Image Data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 22/41 [00:47<00:39,  2.10s/it][A[A

Training on Image Data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [00:49<00:38,  2.14s/it][A[A

Training on Image Data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [00:51<00:36,  2.16s/it][A[A

Training on Image Data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [00:53<00:34,  2.17s/it][A[A

Training on Image Data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 26/41 [00:56<00:32,  2.20s/it][A[A

Training on Image Data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [00:58<00:30,  2.21s/it][A[A

Training on Image Data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [01:00<00:29,  2.24s/it][A[A

Training on Image Data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [01:02<00:26,  2.17s/it][A[A

Training on Image Data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 30/41 [01:04<00:23,  2.10s/it][A[A

Training on Image Data:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [01:06<00:20,  2.09s/it][A[A

Training on Image Data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [01:09<00:19,  2.13s/it][A[A

Training on Image Data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [01:11<00:16,  2.09s/it][A[A

Training on Image Data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 34/41 [01:13<00:14,  2.07s/it][A[A

Training on Image Data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [01:15<00:12,  2.09s/it][A[A

Training on Image Data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [01:17<00:11,  2.20s/it][A[A

Training on Image Data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [01:19<00:08,  2.16s/it][A[A

Training on Image Data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 38/41 [01:21<00:06,  2.11s/it][A[A

Training on Image Data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [01:23<00:04,  2.09s/it][A[A

Training on Image Data:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [01:25<00:02,  2.07s/it][A[A

Training on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:26<00:00,  1.66s/it][A[ATraining on Image Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:26<00:00,  2.11s/it]
Client Image Loss: 0.3527563835789518


Training on Covid Data:   0%|          | 0/31 [00:00<?, ?it/s][A[A

Training on Covid Data:   3%|â–Ž         | 1/31 [00:00<00:19,  1.57it/s][A[A

Training on Covid Data:   6%|â–‹         | 2/31 [00:01<00:21,  1.35it/s][A[A

Training on Covid Data:  10%|â–‰         | 3/31 [00:02<00:20,  1.37it/s][A[A

Training on Covid Data:  13%|â–ˆâ–Ž        | 4/31 [00:02<00:19,  1.39it/s][A[A

Training on Covid Data:  16%|â–ˆâ–Œ        | 5/31 [00:03<00:18,  1.40it/s][A[A

Training on Covid Data:  19%|â–ˆâ–‰        | 6/31 [00:04<00:17,  1.42it/s][A[A

Training on Covid Data:  23%|â–ˆâ–ˆâ–Ž       | 7/31 [00:04<00:16,  1.46it/s][A[A

Training on Covid Data:  26%|â–ˆâ–ˆâ–Œ       | 8/31 [00:05<00:15,  1.49it/s][A[A

Training on Covid Data:  29%|â–ˆâ–ˆâ–‰       | 9/31 [00:06<00:15,  1.42it/s][A[A

Training on Covid Data:  32%|â–ˆâ–ˆâ–ˆâ–      | 10/31 [00:07<00:15,  1.38it/s][A[A

Training on Covid Data:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 11/31 [00:07<00:14,  1.41it/s][A[A

Training on Covid Data:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 12/31 [00:08<00:13,  1.46it/s][A[A

Training on Covid Data:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/31 [00:09<00:12,  1.50it/s][A[A

Training on Covid Data:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 14/31 [00:09<00:11,  1.52it/s][A[A

Training on Covid Data:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15/31 [00:10<00:10,  1.55it/s][A[A

Training on Covid Data:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/31 [00:10<00:09,  1.57it/s][A[A

Training on Covid Data:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17/31 [00:11<00:08,  1.58it/s][A[A

Training on Covid Data:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 18/31 [00:12<00:08,  1.59it/s][A[A

Training on Covid Data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/31 [00:12<00:07,  1.59it/s][A[A

Training on Covid Data:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 20/31 [00:13<00:06,  1.59it/s][A[A

Training on Covid Data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 21/31 [00:14<00:06,  1.59it/s][A[A

Training on Covid Data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 22/31 [00:14<00:05,  1.59it/s][A[A

Training on Covid Data:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/31 [00:15<00:05,  1.59it/s][A[A

Training on Covid Data:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 24/31 [00:16<00:04,  1.53it/s][A[A

Training on Covid Data:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 25/31 [00:16<00:04,  1.49it/s][A[A

Training on Covid Data:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/31 [00:17<00:03,  1.51it/s][A[A

Training on Covid Data:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 27/31 [00:18<00:02,  1.50it/s][A[A

Training on Covid Data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 28/31 [00:18<00:01,  1.51it/s][A[A

Training on Covid Data:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 29/31 [00:19<00:01,  1.52it/s][A[A

Training on Covid Data:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 30/31 [00:19<00:00,  1.55it/s][A[A

Training on Covid Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:20<00:00,  1.95it/s][A[ATraining on Covid Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:20<00:00,  1.54it/s]
Client Covid Loss: 0.36729484219704905


Training on Covid Data:   0%|          | 0/31 [00:00<?, ?it/s][A[A

Training on Covid Data:   3%|â–Ž         | 1/31 [00:00<00:18,  1.61it/s][A[A

Training on Covid Data:   6%|â–‹         | 2/31 [00:01<00:18,  1.60it/s][A[A

Training on Covid Data:  10%|â–‰         | 3/31 [00:01<00:17,  1.60it/s][A[A